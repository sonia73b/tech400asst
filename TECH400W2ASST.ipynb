{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HJi5SX6fmvZYPtzmtXvxdSs8pHLismYx",
      "authorship_tag": "ABX9TyNKlu+M6ONRTJFjHmtq6xLI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonia73b/tech400w2asst/blob/main/TECH400W2ASST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34--PcCXyz-7",
        "outputId": "e9b9e6bf-cf55-4338-a7e1-8f490225c0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download (\"stopwords\")\n",
        "nltk.download (\"punkt\")\n",
        "nltk.download (\"wordnet\")\n",
        "nltk.download (\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, string, logging, re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "8kHtGGmjy47b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def  load_text_files (folder_path ):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "\n",
        "    print(f\"Scanning folder: {folder_path}\")\n",
        "    for filename in os.listdir(folder_path):\n",
        "        print(f\"Found file: {filename}\")\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                data[doc_id] = content\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                print(f\"Loaded doc_id {doc_id} -> {filename}\")\n",
        "                doc_id += 1\n",
        "\n",
        "    print(f\"Total files loaded: {len(data)}\")\n",
        "    return data, doc_id_to_filename\n"
      ],
      "metadata": {
        "id": "JRIDv0_bzAhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if len(word) > 1]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "kub7Hn3VzBou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(data):\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()\n",
        "\n",
        "    for doc_id, content in data.items():\n",
        "        tokens = clean_text(content)\n",
        "        for token in tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1\n",
        "\n",
        "    return inverted_index, term_frequencies\n"
      ],
      "metadata": {
        "id": "KnVRQJ2tzFBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, doc_id_to_filename):\n",
        "    query = query.lower()\n",
        "    tokens = query.split()\n",
        "    result_set = set()\n",
        "\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['', 'or', 'not', 'and']]\n",
        "        if all(term in inverted_index for term in terms):\n",
        "            result_set = inverted_index[terms[0]].copy()\n",
        "            for term in terms[1:]:\n",
        "                result_set &= inverted_index[term]\n",
        "    elif 'or' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['', 'or', 'not', 'and']]\n",
        "        for term in terms:\n",
        "            if term in inverted_index:\n",
        "                result_set |= inverted_index[term]\n",
        "    elif 'not' in tokens:\n",
        "        term = tokens[1]\n",
        "        all_docs = set(doc_id_to_filename.keys())\n",
        "        if term in inverted_index:\n",
        "            result_set = all_docs - inverted_index[term]\n",
        "        else:\n",
        "            result_set = all_docs\n",
        "    else:\n",
        "        if query in inverted_index:\n",
        "            result_set = inverted_index[query]\n",
        "\n",
        "    result_filenames = [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]\n",
        "    logging.info(f\"Query '{query}' resulted in: {result_filenames}\")\n",
        "    return result_filenames\n",
        "\n"
      ],
      "metadata": {
        "id": "4KdGCcTVzHU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_queries_file(term_frequencies, output_file=\"queries.txt\", top_n=5):\n",
        "    queries = [\"EHR AND patient\", \"ICD-10 OR SNOMED\", \"NOT telehealth\"]\n",
        "    with open(\"queries.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for q in queries:\n",
        "            f.write(q + \"\\n\")\n",
        "    print(\"Generated queries file: queries.txt\")"
      ],
      "metadata": {
        "id": "r7X3WibfzJ0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    folder_path = r\"/content/drive/MyDrive/TECH400W2ASST\"\n",
        "\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    for doc_id, content in data.items():\n",
        "        tokens = clean_text(content)\n",
        "        print(f\"Doc {doc_id} cleaned tokens:\", tokens[:20])\n",
        "\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "    print(\"Sample inverted index keys:\", list(inverted_index.keys())[:20])\n",
        "\n",
        "    generate_queries_file(term_frequencies)\n",
        "\n",
        "    queries = [\"EHR AND patient\", \"ICD-10 OR SNOMED\", \"NOT telehealth\"]\n",
        "\n",
        "    with open(\"query_results.txt\", 'w', encoding='utf-8') as result_file:\n",
        "        for query in queries:\n",
        "            result = boolean_query(query, inverted_index, doc_id_to_filename)\n",
        "            result_str = f\"Results for '{query}': {result}\\n\"\n",
        "            print(result_str)\n",
        "            result_file.write(result_str)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6LwOo31zMiN",
        "outputId": "4f289532-b38a-4931-a346-fde66a2b0365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning folder: /content/drive/MyDrive/TECH400W2ASST\n",
            "Found file: Electronic Health Records (EHR).txt\n",
            "Loaded doc_id 0 -> Electronic Health Records (EHR).txt\n",
            "Found file: Interoperability.txt\n",
            "Loaded doc_id 1 -> Interoperability.txt\n",
            "Found file: Virtualcare.txt\n",
            "Loaded doc_id 2 -> Virtualcare.txt\n",
            "Found file: Clinical Decision Support Systems.txt\n",
            "Loaded doc_id 3 -> Clinical Decision Support Systems.txt\n",
            "Found file: HIPAA.txt\n",
            "Loaded doc_id 4 -> HIPAA.txt\n",
            "Found file: Health data analytics.txt\n",
            "Loaded doc_id 5 -> Health data analytics.txt\n",
            "Found file: E-prescribing.txt\n",
            "Loaded doc_id 6 -> E-prescribing.txt\n",
            "Found file: The Internet of Medical Things.txt\n",
            "Loaded doc_id 7 -> The Internet of Medical Things.txt\n",
            "Found file: Clinical coding standards.txt\n",
            "Loaded doc_id 8 -> Clinical coding standards.txt\n",
            "Found file: Change management.txt\n",
            "Loaded doc_id 9 -> Change management.txt\n",
            "Total files loaded: 10\n",
            "Doc 0 cleaned tokens: ['electronic', 'health', 'record', 'ehr', 'are', 'digital', 'version', 'of', 'patient', 'medical', 'file', 'replacing', 'traditional', 'paper', 'chart', 'they', 'allow', 'clinician', 'to', 'view']\n",
            "Doc 1 cleaned tokens: ['hl7', 'fhir', 'is', 'standardized', 'framework', 'that', 'enables', 'secure', 'data', 'exchange', 'between', 'hospital', 'laboratory', 'and', 'mobile', 'health', 'application', 'it', 'us', 'rest']\n",
            "Doc 2 cleaned tokens: ['telehealth', 'enables', 'remote', 'medical', 'consultation', 'using', 'video', 'call', 'chat', 'platform', 'and', 'remote', 'monitoring', 'tool', 'it', 'is', 'especially', 'beneficial', 'for', 'patient']\n",
            "Doc 3 cleaned tokens: ['clinical', 'decision', 'support', 'system', 'cdss', 'provide', 'timely', 'alert', 'reminder', 'and', 'treatment', 'recommendation', 'to', 'clinician', 'these', 'system', 'help', 'detect', 'drug', 'interaction']\n",
            "Doc 4 cleaned tokens: ['hipaa', 'ensures', 'that', 'patient', 'protected', 'health', 'information', 'phi', 'is', 'handled', 'securely', 'and', 'confidentially', 'healthcare', 'organization', 'must', 'restrict', 'access', 'to', 'patient']\n",
            "Doc 5 cleaned tokens: ['health', 'data', 'analytics', 'transforms', 'raw', 'clinical', 'data', 'into', 'actionable', 'insight', 'predictive', 'model', 'use', 'patient', 'data', 'to', 'identify', 'individual', 'at', 'risk']\n",
            "Doc 6 cleaned tokens: ['eprescribing', 'allows', 'physician', 'to', 'send', 'digital', 'prescription', 'directly', 'to', 'pharmacy', 'the', 'system', 'check', 'for', 'allergy', 'incorrect', 'dosage', 'or', 'harmful', 'drug']\n",
            "Doc 7 cleaned tokens: ['the', 'internet', 'of', 'medical', 'thing', 'iomt', 'connects', 'wearable', 'device', 'smart', 'medical', 'monitor', 'and', 'sensor', 'to', 'healthcare', 'network', 'device', 'such', 'a']\n",
            "Doc 8 cleaned tokens: ['clinical', 'coding', 'standard', 'such', 'a', 'icd10', 'loinc', 'and', 'snomed', 'ct', 'classify', 'diagnosis', 'procedure', 'and', 'laboratory', 'result', 'standardized', 'terminology', 'ensures', 'that']\n",
            "Doc 9 cleaned tokens: ['change', 'management', 'is', 'critical', 'in', 'healthcare', 'it', 'project', 'to', 'ensure', 'successful', 'system', 'adoption', 'staff', 'need', 'proper', 'training', 'to', 'understand', 'new']\n",
            "Sample inverted index keys: ['electronic', 'health', 'record', 'ehr', 'are', 'digital', 'version', 'of', 'patient', 'medical', 'file', 'replacing', 'traditional', 'paper', 'chart', 'they', 'allow', 'clinician', 'to', 'view']\n",
            "Generated queries file: queries.txt\n",
            "Results for 'EHR AND patient': ['Electronic Health Records (EHR).txt', 'Interoperability.txt']\n",
            "\n",
            "Results for 'ICD-10 OR SNOMED': ['Clinical coding standards.txt']\n",
            "\n",
            "Results for 'NOT telehealth': ['Electronic Health Records (EHR).txt', 'Interoperability.txt', 'Clinical Decision Support Systems.txt', 'HIPAA.txt', 'Health data analytics.txt', 'E-prescribing.txt', 'The Internet of Medical Things.txt', 'Clinical coding standards.txt', 'Change management.txt']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPRr_aO2zO4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}